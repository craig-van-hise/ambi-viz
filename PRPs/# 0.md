
## 5. Phased Implementation Plan (The Agent Instructions)

**To the Autonomous Coding Agent:**

Proceed with the implementation of AmbiViz in the following five distinct phases. Execute the Validation Checkpoints at the end of each phase before proceeding.

### 5.1 Phase 1: The Signal Chain and Data Ingestion

**Goal:** Build a robust pipeline that loads an Ambisonic file, decodes it for monitoring, and taps the raw data for visualization.

1. **Input Handling:** Implement a drag-and-drop file loader supporting .wav and .ambix containers. Parse the file headers to determine the Ambisonic Order (). If the file is 4 channels, ; if 9 channels, ; if 16 channels, .
2. **Audio Context & Graph:** Initialize an AudioContext. Create a buffer source.
3. **Binaural Decoding:** Instantiate a `JSAmbisonics.binDecoder` set to the detected order. Load a standard HRTF set (use a generic SOFA file loaded via `JSAmbisonics.HRIRloader_ircam`).<sup>6</sup> Connect the source to the decoder, and the decoder to the destination.
4. **Raw Data Extraction:**
* Instantiate a `JSAmbisonics.intensityAnalyser` (or `powermapAnalyser`). Connect the source to this analyzer.
* **CRITICAL:** Do not use the analyzer's built-in canvas drawing methods. You must access the raw coefficient data. If the library exposes a `.coefficients` property or a method to `getEnergy()`, map this to a `Float32Array`.


5. **Temporal Smoothing (Ballistics):** Raw audio coefficients fluctuate at the sample rate, creating a jittery, unusable visualization. Implement a temporal smoothing filter in the JavaScript update loop using the following ballistic logic:
* Define `attack = 0.1` and `release = 0.9`.
* For each coefficient :
* Use the attack coefficient if  (rising edge), and release if falling. This mimics the ballistics of a PPM meter, giving the visual mass "weight".<sup>1</sup>



**Validation Checkpoint 1:**

* **Test:** Load a "rotating noise" calibration file (mono source panning 360 degrees).<sup>13</sup>
* **Verify:** Audio monitors correctly binaurally (sound moves around head).
* **Verify:** Console logs of the smoothed coefficients show values peaking sequentially in the Y, X, and Z channels as the source rotates.

### 5.2 Phase 2: The Shader Core and Basis Summation

**Goal:** Implement the mathematical engine within GLSL to reconstruct the sound field shape from the coefficients.

1. **Shader Infrastructure:** Create a `THREE.ShaderMaterial`. Define the following Uniforms:
* `uCoeffs`: Float array of size  (e.g., 16 floats for 3rd order).
* `uOrder`: Integer (the order ).
* `uOpacity`: Float (master gain).
* `uColorMap`: Sampler2D (heatmap gradient texture).


2. **Spherical Harmonics in GLSL:** You must manually implement the real-valued Spherical Harmonic basis functions () using **ACN/SN3D** standards.
* *Reference:*  (omni). , ,  (dipoles).
* Implement functions up to at least Order 3.
* **Optimization:** Do not use heavy branching. Pre-compute the polynomial forms.<sup>15</sup>


3. **The evaluateHOA Function:** Write a GLSL function `float evaluateHOA(vec3 dir)` that:
* Takes a normalized direction vector.
* Iterates from index  to .
* Sums the product of the coefficient `uCoeffs[q]` and the basis function .
* Returns the squared magnitude of the sum (Energy): . Visual mass must be positive.<sup>1</sup>



**Validation Checkpoint 2:**

* **Test:** Temporarily map this shader to a standard SphereMesh using vertex displacement (`position += normal * evaluateHOA(normal)`).
* **Verify:** Playing the rotating calibration file should result in a "spiky ball" where the spike points in the direction of the sound. If the spike points opposite, check the polarity of your basis functions (SN3D vs N3D).<sup>13</sup>

### 5.3 Phase 3: The Volumetric Raymarching Engine

**Goal:** Discard the mesh and implement the volumetric "cloud" renderer.

1. **Scene Setup:** Remove the sphere mesh. Add a `PlaneBufferGeometry(2, 2)` (full-screen quad) or a `BoxGeometry` encompassing the camera. Apply the shader.
2. **Raymarching Loop:** In the Fragment Shader `main()`:
* Calculate `rayDir`: The normalized vector from the camera position to the pixel coordinate in world space.
* Initialize `accumulatedColor = vec3(0)` and `accumulatedAlpha = 0`.
* Loop `i` from 0 to MAX_STEPS (e.g., 64).
* Current Position `p = cameraPosition + rayDir * t`.


3. **Density Evaluation:** Inside the loop:
* Call `float density = evaluateHOA(normalize(p))`.
* **Radial Falloff:** Multiply density by `smoothstep(radius, 0.0, length(p))`. This ensures the cloud fades out at a finite distance, creating a "ball of fog" rather than an infinite field.
* **Noise Floor:** If `density < 0.001`, continue (skip accumulation) to keep silence transparent.


4. **Beer-Lambert Accumulation:** Approximate light absorption:
* `float stepDensity = density * stepSize;`
* `accumulatedAlpha += stepDensity;`
* `vec3 heatColor = texture2D(uColorMap, density).rgb;`
* `accumulatedColor += heatColor * stepDensity * (1.0 - accumulatedAlpha);`
* **Early Exit:** `if (accumulatedAlpha >= 1.0) break;`.<sup>16</sup>



**Validation Checkpoint 3:**

* **Test:** Play a pulsed sound.
* **Verify:** The screen should fill with a 3D glowing "fog" in the direction of the source. The center should be dense/white, fading to transparent.
* **Performance:** Check frame rate. If < 60fps, reduce MAX_STEPS or increase stepSize.<sup>17</sup>

### 5.4 Phase 4: Psychoacoustic Feature Extraction

**Goal:** Implement the "Distance" and "Spread" visualizations.

**4.1 Distance (NFE Implementation):**

1. **Dual-Band Analysis:** In the AudioWorklet, create a parallel signal path. Apply a **Low-Pass Filter (LPF)** (Biquad) at **250Hz** to the signal.
2. **Ratio Calculation:** On this low-passed stream, calculate:
* Pressure Energy:
* Velocity Energy:
* .<sup>1</sup>


3. **Uniform Passing:** Pass  to the shader as `uDistance`.
4. **Visual Logic:** In the GLSL `evaluateHOA` function, use `uDistance` to modify the color lookup coordinate.
* If `uDistance > 1.5` (Near), shift the color lookup toward White/Orange.
* Optionally, use `uDistance` to visually scale the Radial Falloff, making the cloud appear physically smaller and closer.



**4.2 Spread (Diffuseness):**

1. **Vector Calculation:** In the main JS loop, calculate the Energy Vector:
* , ,  (Active Intensity).
* (Total Energy).
* .<sup>11</sup>


2. **Diffuseness:** . Pass as `uDiffuseness`.
3. **Shader Logic:** Map `uDiffuseness` to a "Jitter" or "Scatter" term in the raymarching loop.
* High Diffuseness: Add noise to the ray sampling position `p`. This blurs the density field, turning a focused beam into a diffuse fog.<sup>1</sup>



**Validation Checkpoint 4:**

* **Test:** Play a "Voice (Close Mic)" vs. "Choir (Far/Reverb)" recording.
* **Verify:** The voice should trigger the "Heat" effect (NFE). The choir should appear as a diffuse, low-density fog ().

### 5.5 Phase 5: Interaction and Navigation

**Goal:** Implement "Inside-Out" navigation.

1. **Camera Rig:** Place the camera at .
2. **Desktop Controls:** Implement `PointerLockControls`.
* Map Mouse X to Camera Yaw (Y-rotation).
* Map Mouse Y to Camera Pitch (X-rotation).
* **Constraint:** Disable translation. The user must remain at the "sweet spot" (origin) to perceive the field correctly.<sup>18</sup>


3. **Mobile/VR Controls:** Implement `DeviceOrientationControls`.
* Use the `deviceorientation` event to map the phone's gyroscope quaternion to the camera. This creates a "Magic Window" where moving the phone reveals the sound field.<sup>19</sup>



**Validation Checkpoint 5:**

* **Test:** Load a source at "Front Left".
* **Verify:** Rotate camera to look Front Left. The visual mass should center in the screen.

## 6. Validation and Quality Assurance Protocols

The agent must implement a debug loop that runs these checks continuously:

| Checkpoint | Logic | Failure Action |
| --- | --- | --- |
| **Order Integrity** | `coeffArray.length == (order+1)^2` | Throw Error: "Channel Mismatch". Stop Render. |
| **Silence Clip** | `if (Energy < -60dB) Opacity = 0` | Force Alpha to 0 (Prevent ghosting). |
| **NFE Safety** | `if (LowFreqEnergy < Threshold)` | Set `uDistance = 0` (Prevent div/0 errors). |
| **Frame Timing** | Measure delta time between frames. | If fps < 30, increase `STEP_SIZE` in shader. |

## 7. Technical Appendix: Algorithms and GLSL Reference

### 7.1 Mathematical Reference: The NFE Ratio

For the agent's reference in **Phase 4**:

To calculate the "Distance" metric, compare the energy of the velocity components to the pressure component in the low-frequency band ().

Where:

* (Energy of the filtered velocity channels)
* (Energy of the filtered pressure channel)

If , the source is Far. If , the source is Near.<sup>1</sup>

### 7.2 GLSL Reference: The Density Function

This function evaluates the spherical harmonic series to determine "how much sound" is at a specific point in space.

```glsl
// Uniforms: uCoeffs (array), uOrder (int)
float getDensity(vec3 p) {
    vec3 dir = normalize(p);
    float r = length(p);
    
    // 1. Convert to Spherical Coordinates
    // Note: Verify coordinate system matches Three.js (Y-up)
    float theta = acos(dir.z); // Elevation (0 to PI)
    float phi = atan(dir.y, dir.x); // Azimuth (-PI to PI)
    
    // 2. Evaluate Spherical Harmonics (Dynamic Loop)
    float shEnergy = 0.0;
    int index = 0;
    
    // Loop through Orders (n) and Degrees (m)
    // Note: In production, unroll this loop or use fixed-size arrays for performance
    for (int n = 0; n <= 3; n++) { // Hardcoded max order 3 for loop safety
        if (n > uOrder) break;
        for (int m = -n; m <= n; m++) {
            // Get Basis Function value Y_n^m(theta, phi)
            // Agent must implement getSHBasis() based on SN3D formulas
            float Y = getSHBasis(n, m, theta, phi); 
            
            // Sum weighted by coefficient
            shEnergy += uCoeffs[index] * Y;
            index++;
        }
    }
    
    // 3. Squaring for Energy (Visual Mass must be positive)
    float density = shEnergy * shEnergy;
    
    // 4. Radial Falloff (The "Ball" shape)
    // Limits the infinite angular field to a finite visual radius
    float falloff = smoothstep(1.0, 0.0, r / 5.0); // 5.0 unit radius
    
    return density * falloff;
}

```

### 7.3 GLSL Reference: The Raymarching Loop

This loop accumulates density to render the volume.<sup>1</sup>

```glsl
void main() {
    vec3 rayOrigin = cameraPosition;
    vec3 rayDir = normalize(vWorldPos - cameraPosition);
    
    float totalDensity = 0.0;
    vec3 accumulatedColor = vec3(0.0);
    float t = 0.0; // Distance along ray
    
    // Raymarching Steps
    for(int i = 0; i < 64; i++) {
        vec3 p = rayOrigin + rayDir * t;
        float dens = getDensity(p);
        
        // Accumulate if density is significant
        if(dens > 0.01) {
            float absorption = dens * uStepSize;
            totalDensity += absorption;
            
            // Heatmap Color Mapping based on density
            // mapHeatmap() should lookup texture based on dens
            vec3 color = mapHeatmap(dens); 
            
            // Beer's Law Approximation
            accumulatedColor += color * absorption * (1.0 - totalDensity);
        }
        
        // Early Exit (Opaque)
        if(totalDensity >= 1.0) break;
        
        t += uStepSize;
    }
    
    gl_FragColor = vec4(accumulatedColor, totalDensity);
}

```

## 8. Conclusion

The AmbiViz project represents a paradigm shift in audio analysis. By rigorously applying the physics of **Volumetric Raymarching** and the psychoacoustic math of the **Near-Field Effect**, this system will provide audio engineers with a visualization that finally matches their internal mental model of a sound field. It is not a graph; it is a space.

The instructions provided in this PRP are designed to guide an autonomous agent through the complexities of cross-thread audio processing, custom shader mathematics, and standardization compliance. Adherence to the **ACN/SN3D** standard and the **Volumetric** metaphor is paramount for the success of this tool. Proceed with the implementation of the Signal Chain.

**End of Product Requirements Prompt.**
