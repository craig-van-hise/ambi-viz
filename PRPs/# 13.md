
## Product Requirements Prompt (PRP): Low-Latency Predictive Head Tracking

You are an elite Senior Feature Architect and developer agent operating within the Google Antigravity IDE (v1.14.2).

### A. Feature Context

You are adding Predictive Webcam Head Tracking to the existing AmbiViz codebase. Your goal is to implement this functionality while maintaining the integrity of the current system, specifically the `obr.wasm` WebAudio processing pipeline. The core objective is to achieve a motion-to-sound latency of below 60ms. Standard webcams and inference models introduce an inherent 90ms+ delay. Therefore, you must establish a lockless, multithreaded architecture (Web Worker communicating to an AudioWorklet via SharedArrayBuffer) and utilize advanced Asynchronous Timewarp prediction mathematics (the 1 Euro Filter and an Error-State Kalman Filter) to predict the user's head orientation 45ms into the future.

### B. Configuration Updates

* 
**Strict Memory & Architecture:** Utilize standard coding practices, maintaining strict separation of concerns between DOM manipulation (main thread), machine learning inference (Web Worker), and spatial audio processing (AudioWorklet). Ensure the `AudioWorkletProcessor` remains entirely non-blocking (no `Atomics.wait()` or predictable garbage collection).


* 
**Dependencies:** You must install the following packages before writing implementation code:


* 
`npm install @mediapipe/tasks-vision` (Required for the Face Landmarker API).


* 
`npm install -D gl-matrix` (Required for robust, optimized Quaternion and Matrix mathematics).




* 
**Security & Memory:** Ensure cross-origin isolation headers (`Cross-Origin-Opener-Policy: same-origin`, `Cross-Origin-Embedder-Policy: require-corp`) are configured in the application's development server to permit SharedArrayBuffer allocation.



### C. Phased Implementation & TDD Strategy

You are required to build this feature sequentially using Test-Driven Development (TDD) to ensure the highly sensitive audio thread is never compromised by blocking operations.

#### Phase 1: The "Walking Skeleton", Camera Init & Tests

* 
**Objective:** Establish the multithreaded plumbing, user media capture, and lockless memory synchronization.


* **Tasks:**
* 
**SAB Schema:** Define the SharedArrayBuffer memory schema precisely. Instantiate an `Int32Array` for Atomics flags (sequence numbers) and a `Float32Array` for the quaternion kinematics. Write the TypeScript interfaces required for the Main Thread, Worker, and Worklet message passing.


* 
**Audio Integration:** Write failing unit tests that assert the `AudioWorkletProcessor` can successfully read mock atomic data from the SAB without using `Atomics.wait()` or blocking the thread. Connect the SAB to the `obr.wasm` rotation API inside the AudioWorklet's `process()` method.


* **Main Thread Camera Capture:** Implement DOM manipulation in the main thread to request user webcam permissions via `navigator.mediaDevices.getUserMedia`, instantiate a hidden HTML `<video>` element, and pass the live video stream to the Web Worker.
* 
**Vision Worker:** Implement the MediaPipe FaceLandmarker initialization in a Web Worker, establish a loop synchronized to the incoming video element stream , extract the `pose_transform_matrix`, convert the $3\times3$ rotational submatrix to a unit quaternion , and write it to the SAB using `Atomics.store()`.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the raw webcam head tracking in the browser.**



#### Phase 2: Lightweight Predictive Tracking (Velocity Extrapolation & 1 Euro Filter)

* 
**Objective:** Implement Dead Reckoning and adaptive signal smoothing to combat static jitter and latency.


* **Tasks:**
* Write failing tests for a `OneEuroFilter` class, asserting that it applies heavy smoothing (low cutoff) at low simulated velocities, and minimal smoothing (high cutoff) at high velocities.


* Implement the 1 Euro Filter mathematical algorithm to smooth the incoming raw quaternions from MediaPipe.


* Calculate the Delta Quaternion between consecutive smoothed frames to derive the angular velocity vector $\omega$.


* Implement a constant velocity extrapolation algorithm to mathematically rotate the current quaternion into the future based on the extracted angular velocity .


* Write this predicted quaternion to the `QUAT_PRED` indices in the SAB.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the smoothed tracking implementation.**



#### Phase 3: Advanced Predictive Tracking (Error-State Kalman Filter)

* 
**Objective:** Replace naive velocity extrapolation with a robust 3DOF Error-State Kalman Filter (ESKF) to eliminate post-movement overshoot and appropriately model human biomechanical momentum.


* **Tasks:**
* Write unit tests for an ESKF class, specifically verifying that the nominal state successfully maintains a strict unit-norm constraint ($||q||=1$) after an error-state vector is injected into it.


* Implement the ESKF matrix mathematics. Define the continuous nominal state (quaternion and angular velocity) and the infinitesimal error state (a 3-dimensional rotation vector $\delta\theta$ residing in the tangent space, avoiding covariance singularity) .


* Implement the discrete-time prediction (time update) step. Calculate the Jacobian matrix $F_k$ of the non-linear system dynamics to propagate the covariance matrix $P$ forward in time alongside the nominal quaternion .


* Implement the measurement (correction) update step. Calculate the Kalman Gain $K_k$, compute the measurement residual against the noisy webcam data, and multiply to generate an observation of the error state.


* Inject the computed error state back into the nominal state using quaternion multiplication, and explicitly reset the error state to zero.


* Output the ESKF's predicted quaternion forward in time by $\tau=45ms$ to the SAB.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the final predictive head tracking pipeline.**



#### Phase 4: Telemetry, UI Exigency & Real-Time ESKF Tuning

* **Objective:** Expose the internal Error-State Kalman Filter (ESKF) covariance matrices and prediction horizon to the Main Thread UI. This will allow for real-time, lockless empirical tuning to bridge the 80-105ms pipeline delay and achieve the sub-60ms motion-to-sound latency budget.
* **Tasks:**
* **Worker Message Passing:** Implement a non-blocking `message` event listener inside the Vision Worker. It must accept a payload containing `{ tau, R_scalar, Q_scalar }` and update the ESKF's internal state safely without interrupting the MediaPipe inference loop or the SharedArrayBuffer writes.
* **Mathematical Hookup:** Ensure the ESKF dynamically recalculates its noise matrices upon receiving an update. Map `R_scalar` to the $3\times3$ measurement noise covariance matrix (e.g., $R = \text{R\_scalar} \cdot I_{3\times3}$) and `Q_scalar` to the process noise covariance matrix $Q$.
* **Main Thread UI Controls:** Extend the existing DOM viewer with the following specific inputs. **Crucially, the covariance sliders must be mathematically mapped to logarithmic scales** in the JavaScript logic, as linear scaling will fail to capture the required orders of magnitude:
* **Prediction Horizon ($\tau$):** Linear slider. Range: 0ms to 150ms. Step: 5ms. (Used to precisely offset the inherent pipeline delay).
* **Measurement Noise ($R$):** Logarithmic slider. Range: 0.0001 to 0.1. (Controls trust in the raw, noisy MediaPipe webcam data).
* **Process Noise ($Q$):** Logarithmic slider. Range: 0.000001 to 0.01. (Controls trust in the constant-velocity biomechanical mathematical model).


* **Visual Debugging (Ghosting):** Implement a dual-render state in the UI viewer. Render a semi-transparent "ghost" representation of the *raw* MediaPipe quaternion alongside the solid representation of the *ESKF predicted* quaternion. This provides immediate visual feedback on the prediction horizon's overshoot and the filter's jitter suppression.


* **Regression Mandate:** Before marking this phase complete, you must run ALL existing tests to ensure the new message-passing architecture has not broken the AudioWorklet's lockless constraints or the ESKF's unit-norm mathematical proofs.
* **Checkpoint Law:** STOP. You are strictly forbidden from moving forward until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to begin empirically tuning the $Q$, $R$, and $\tau$ sliders in the browser to hit the 60ms latency target.**

