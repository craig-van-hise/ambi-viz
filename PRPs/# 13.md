
## Product Requirements Prompt (PRP): Low-Latency Predictive Head Tracking

You are an elite Senior Feature Architect and developer agent operating within the Google Antigravity IDE (v1.14.2).

### A. Feature Context

You are adding Predictive Webcam Head Tracking to the existing AmbiViz codebase. Your goal is to implement this functionality while maintaining the integrity of the current system, specifically the `obr.wasm` WebAudio processing pipeline. The core objective is to achieve a motion-to-sound latency of below 60ms. Standard webcams and inference models introduce an inherent 90ms+ delay. Therefore, you must establish a lockless, multithreaded architecture (Web Worker communicating to an AudioWorklet via SharedArrayBuffer) and utilize advanced Asynchronous Timewarp prediction mathematics (the 1 Euro Filter and an Error-State Kalman Filter) to predict the user's head orientation 45ms into the future.

### B. Configuration Updates

* 
**Strict Memory & Architecture:** Utilize standard coding practices, maintaining strict separation of concerns between DOM manipulation (main thread), machine learning inference (Web Worker), and spatial audio processing (AudioWorklet). Ensure the `AudioWorkletProcessor` remains entirely non-blocking (no `Atomics.wait()` or predictable garbage collection).


* 
**Dependencies:** You must install the following packages before writing implementation code:


* 
`npm install @mediapipe/tasks-vision` (Required for the Face Landmarker API).


* 
`npm install -D gl-matrix` (Required for robust, optimized Quaternion and Matrix mathematics).




* 
**Security & Memory:** Ensure cross-origin isolation headers (`Cross-Origin-Opener-Policy: same-origin`, `Cross-Origin-Embedder-Policy: require-corp`) are configured in the application's development server to permit SharedArrayBuffer allocation.



### C. Phased Implementation & TDD Strategy

You are required to build this feature sequentially using Test-Driven Development (TDD) to ensure the highly sensitive audio thread is never compromised by blocking operations.

#### Phase 1: The "Walking Skeleton", Camera Init & Tests

* 
**Objective:** Establish the multithreaded plumbing, user media capture, and lockless memory synchronization.


* **Tasks:**
* 
**SAB Schema:** Define the SharedArrayBuffer memory schema precisely. Instantiate an `Int32Array` for Atomics flags (sequence numbers) and a `Float32Array` for the quaternion kinematics. Write the TypeScript interfaces required for the Main Thread, Worker, and Worklet message passing.


* 
**Audio Integration:** Write failing unit tests that assert the `AudioWorkletProcessor` can successfully read mock atomic data from the SAB without using `Atomics.wait()` or blocking the thread. Connect the SAB to the `obr.wasm` rotation API inside the AudioWorklet's `process()` method.


* **Main Thread Camera Capture:** Implement DOM manipulation in the main thread to request user webcam permissions via `navigator.mediaDevices.getUserMedia`, instantiate a hidden HTML `<video>` element, and pass the live video stream to the Web Worker.
* 
**Vision Worker:** Implement the MediaPipe FaceLandmarker initialization in a Web Worker, establish a loop synchronized to the incoming video element stream , extract the `pose_transform_matrix`, convert the $3\times3$ rotational submatrix to a unit quaternion , and write it to the SAB using `Atomics.store()`.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the raw webcam head tracking in the browser.**



#### Phase 2: Lightweight Predictive Tracking (Velocity Extrapolation & 1 Euro Filter)

* 
**Objective:** Implement Dead Reckoning and adaptive signal smoothing to combat static jitter and latency.


* **Tasks:**
* Write failing tests for a `OneEuroFilter` class, asserting that it applies heavy smoothing (low cutoff) at low simulated velocities, and minimal smoothing (high cutoff) at high velocities.


* Implement the 1 Euro Filter mathematical algorithm to smooth the incoming raw quaternions from MediaPipe.


* Calculate the Delta Quaternion between consecutive smoothed frames to derive the angular velocity vector $\omega$.


* Implement a constant velocity extrapolation algorithm to mathematically rotate the current quaternion into the future based on the extracted angular velocity .


* Write this predicted quaternion to the `QUAT_PRED` indices in the SAB.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the smoothed tracking implementation.**



#### Phase 3: Advanced Predictive Tracking (Error-State Kalman Filter)

* 
**Objective:** Replace naive velocity extrapolation with a robust 3DOF Error-State Kalman Filter (ESKF) to eliminate post-movement overshoot and appropriately model human biomechanical momentum.


* **Tasks:**
* Write unit tests for an ESKF class, specifically verifying that the nominal state successfully maintains a strict unit-norm constraint ($||q||=1$) after an error-state vector is injected into it.


* Implement the ESKF matrix mathematics. Define the continuous nominal state (quaternion and angular velocity) and the infinitesimal error state (a 3-dimensional rotation vector $\delta\theta$ residing in the tangent space, avoiding covariance singularity) .


* Implement the discrete-time prediction (time update) step. Calculate the Jacobian matrix $F_k$ of the non-linear system dynamics to propagate the covariance matrix $P$ forward in time alongside the nominal quaternion .


* Implement the measurement (correction) update step. Calculate the Kalman Gain $K_k$, compute the measurement residual against the noisy webcam data, and multiply to generate an observation of the error state.


* Inject the computed error state back into the nominal state using quaternion multiplication, and explicitly reset the error state to zero.


* Output the ESKF's predicted quaternion forward in time by $\tau=45ms$ to the SAB.




* 
**Regression Mandate:** Before marking a phase complete, you must run ALL existing tests to ensure this new feature has not broken previous functionality.


* **Checkpoint Law:** STOP. You are strictly forbidden from moving to the next phase until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the final predictive head tracking pipeline.**



#### Phase 4: Telemetry, UI Exigency & Real-Time ESKF Tuning

* **Objective:** Expose the internal Error-State Kalman Filter (ESKF) covariance matrices and prediction horizon to the Main Thread UI. This will allow for real-time, lockless empirical tuning to bridge the 80-105ms pipeline delay and achieve the sub-60ms motion-to-sound latency budget.
* **Tasks:**
* **Worker Message Passing:** Implement a non-blocking `message` event listener inside the Vision Worker. It must accept a payload containing `{ tau, R_scalar, Q_scalar }` and update the ESKF's internal state safely without interrupting the MediaPipe inference loop or the SharedArrayBuffer writes.
* **Mathematical Hookup:** Ensure the ESKF dynamically recalculates its noise matrices upon receiving an update. Map `R_scalar` to the $3\times3$ measurement noise covariance matrix (e.g., $R = \text{R\_scalar} \cdot I_{3\times3}$) and `Q_scalar` to the process noise covariance matrix $Q$.
* **Main Thread UI Controls:** Extend the existing DOM viewer with the following specific inputs. **Crucially, the covariance sliders must be mathematically mapped to logarithmic scales** in the JavaScript logic, as linear scaling will fail to capture the required orders of magnitude:
* **Prediction Horizon ($\tau$):** Linear slider. Range: 0ms to 150ms. Step: 5ms. (Used to precisely offset the inherent pipeline delay).
* **Measurement Noise ($R$):** Logarithmic slider. Range: 0.0001 to 0.1. (Controls trust in the raw, noisy MediaPipe webcam data).
* **Process Noise ($Q$):** Logarithmic slider. Range: 0.000001 to 0.01. (Controls trust in the constant-velocity biomechanical mathematical model).


* **Visual Debugging (Ghosting):** Implement a dual-render state in the UI viewer. Render a semi-transparent "ghost" representation of the *raw* MediaPipe quaternion alongside the solid representation of the *ESKF predicted* quaternion. This provides immediate visual feedback on the prediction horizon's overshoot and the filter's jitter suppression.


* **Regression Mandate:** Before marking this phase complete, you must run ALL existing tests to ensure the new message-passing architecture has not broken the AudioWorklet's lockless constraints or the ESKF's unit-norm mathematical proofs.
* **Checkpoint Law:** STOP. You are strictly forbidden from moving forward until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to begin empirically tuning the $Q$, $R$, and $\tau$ sliders in the browser to hit the 60ms latency target.**



### Phase 5: UX Telemetry Tooltips & Audio Transport Control

* **Objective:** Enhance the Main Thread UI to support empirical tuning by adding descriptive tooltips to the ESKF controls. Additionally, implement a non-blocking audio transport mechanism to control the `obr.wasm` playback state without violating the AudioWorklet's strict performance constraints.
* **Tasks:**
* **ESKF Parameter Tooltips:** Implement native HTML `title` attributes or custom CSS hover tooltips for the three ESKF tuning sliders. Use the following exact copy to guide the user's tuning process:
* **Prediction Horizon (Tau):** "Offsets system delay. Increase until audio panning feels instantaneous. If the sound field 'rubber-bands' or overshoots when you abruptly stop your head, decrease this value."
* **Measurement Noise (R):** "Trust in the webcam. Lower = faster response but captures more micro-jitter. Higher = smoother but can feel sluggish. Listen for rapid stutters in the audio field; increase until the stutter disappears."
* **Process Noise (Q):** "Trust in head momentum. Lower = assumes smooth, predictable movement. Higher = better tracking for sudden, erratic head whips. Increase if the audio feels like it drags behind your fast turns."


* **Audio Transport UI:** Implement a simple, accessible transport control section within the DOM viewer containing four distinct buttons: **Play**, **Pause**, **Stop**, and **Loop** (Toggle).
* **AudioWorklet Synchronization:** Wire the transport controls to govern the Web Audio API context and the `obr.wasm` buffer processing.
* **Play/Pause:** Should gracefully resume or suspend the `AudioContext` to save processing power.
* **Stop:** Must suspend the context and reset the internal playback cursor of the audio buffer back to zero.
* **Loop (State):** Must toggle a boolean state that dictates whether the `obr.wasm` playback buffer restarts upon reaching the end of the file. Ensure this state is passed cleanly to the AudioWorklet without using blocking operations.




* **Regression Mandate:** Run all existing tests. Ensure the addition of the transport controls does not introduce any unpredictable garbage collection or blocking behavior within the `AudioWorkletProcessor`'s `process()` method.
* **Checkpoint Law:** STOP. You are strictly forbidden from moving forward until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test the transport controls and read the tooltips.**



### Phase 6: Audio Queue, State Persistence, and UI Polish

* **Objective:** Elevate the user experience from a raw testing environment to a robust tool by implementing an audio track queue, expanding transport controls, persisting tuning states across sessions, and refining the visual viewer.
* **Tasks:**
* **Track Queue & Advanced Drag-and-Drop:** * Implement a visible Track List (Queue) UI positioned immediately above the transport controls.
* Refactor the existing drag-and-drop event handlers to append dropped items to this queue rather than instantly replacing the current buffer.
* Expand the drop handler to accept both individual audio files and entire folders (utilizing `webkitGetAsEntry` or the File System Access API to recursively parse supported audio files within dropped directories).


* **Transport Expansion & Playback Behavior:** * Add **Previous** and **Next** buttons to the transport UI. Wire these to navigate the newly created Track Queue, loading the appropriate buffer into the `obr.wasm` instance.
* **Crucial UX Fix:** Disable auto-play on file drop. The `AudioContext` and playback must remain suspended until the user explicitly clicks "Play" in the transport or triggers a `keydown` event using the Spacebar.


* **State Persistence:** * Implement a lightweight state management system using `localStorage` to save and restore the user's configuration upon page load or refresh.
* The persisted state schema must include: HRTF status, Master Gain, and the three ESKF tuning parameters ($\tau$, $R$, and $Q$).


* **ESKF Parameter Expansion:** * Modify the Process Noise ($Q$) slider in the Main Thread UI. Increase its maximum allowable range from 0.25 to **0.5** to allow the user to test more aggressive tuning against human biomechanical momentum.
* **Spatial UI Polish (Orientation Labels):** * Refactor the orientation labels in the viewer. Attempt to render these labels using transparent, native SVG elements to eliminate the current rectangular backgrounds.
* *Fallback:* If pure SVG implementation conflicts with the current canvas rendering context, alter the depth-sorting/z-index logic so that the rectangular labels render strictly *behind* the spatializer point cloud, ensuring they no longer occlude the data visualization.


* **Regression Mandate:** Before marking this phase complete, you must run ALL existing tests. Specifically verify that recursively parsing large folders does not block the main thread and that `localStorage` retrieval correctly hydrates the Web Worker's ESKF state without triggering race conditions during initialization.
* **Checkpoint Law:** STOP. You are strictly forbidden from moving forward until you run the tests and verify they PASS. **Once tests pass, you must open `localhost` and explicitly notify the user to test dropping a folder, verifying the Spacebar playback toggle, and refreshing the page to confirm ESKF settings persist.**


